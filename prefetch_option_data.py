#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœŸæƒæ•°æ®é¢„ä¸‹è½½è„šæœ¬
Pre-download all option data from CSV signals to cache
ä»CSVä¸­è¯»å–æ‰€æœ‰æœŸæƒä¿¡å·ï¼Œé¢„å…ˆä¸‹è½½ä»ä¿¡å·æ—¶é—´åˆ°åˆ°æœŸæ—¥çš„æ‰€æœ‰å†å²æ•°æ®
"""

import os
import sys
import csv
import pytz
import logging
import requests
import pandas as pd
from pathlib import Path
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Optional
from dotenv import load_dotenv

# Load .env
env_path = Path(__file__).parent / '.env'
load_dotenv(env_path)

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('prefetch_option_data.log', mode='w', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class OptionDataPrefetcher:
    """æœŸæƒæ•°æ®é¢„ä¸‹è½½å™¨"""
    
    BASE_URL = "https://api.polygon.io/v2/aggs/ticker"
    
    def __init__(self, csv_file: str, cache_dir: str):
        """
        åˆå§‹åŒ–é¢„ä¸‹è½½å™¨
        
        Args:
            csv_file: CSVä¿¡å·æ–‡ä»¶è·¯å¾„
            cache_dir: ç¼“å­˜ç›®å½•
        """
        self.csv_file = Path(csv_file)
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        self.api_key = os.getenv('POLYGON_API_KEY')
        if not self.api_key:
            raise ValueError("POLYGON_API_KEY not found in .env file")
        
        self.api_calls = 0
        self.cache_hits = 0
        self.failed_tickers = []
        
        # æ—¶åŒº
        self.cn_tz = pytz.timezone('Asia/Shanghai')
        self.et_tz = pytz.timezone('America/New_York')
        
        logger.info(f"ğŸ“‚ CSVæ–‡ä»¶: {self.csv_file}")
        logger.info(f"ğŸ“ ç¼“å­˜ç›®å½•: {self.cache_dir}")
    
    def load_option_signals(self) -> List[Dict]:
        """
        ä»CSVåŠ è½½æ‰€æœ‰æœŸæƒä¿¡å·
        
        Returns:
            æœŸæƒä¿¡å·åˆ—è¡¨
        """
        logger.info(f"ğŸ“– è¯»å–CSVæ–‡ä»¶: {self.csv_file}")
        signals = []
        
        with open(self.csv_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for idx, row in enumerate(reader, 1):
                try:
                    signal = self._parse_signal(row)
                    if signal:
                        signals.append(signal)
                except Exception as e:
                    logger.debug(f"è¡Œ{idx}è§£æå¤±è´¥: {e}")
                    continue
        
        logger.info(f"âœ… å…±è¯»å– {len(signals)} ä¸ªæœŸæƒä¿¡å·")
        return signals
    
    def _parse_signal(self, row: Dict) -> Optional[Dict]:
        """è§£æCSVè¡Œ"""
        try:
            ticker = row['ticker']
            date_str = row['date']
            time_str = row['time']
            strike = float(row['strike'])
            option_type = row['option_type'].lower()
            expiry_str = row['expiry']
            
            # æ—¶é—´è½¬æ¢ï¼šä¸­å›½æ—¶é—´ â†’ ç¾ä¸œæ—¶é—´
            datetime_str = f"{date_str} {time_str}"
            signal_time_cn = datetime.strptime(datetime_str, '%Y-%m-%d %H:%M:%S')
            signal_time_cn = self.cn_tz.localize(signal_time_cn)
            signal_time_et = signal_time_cn.astimezone(self.et_tz)
            
            # è§£æåˆ°æœŸæ—¥
            expiry_date = datetime.strptime(expiry_str, '%Y-%m-%d').date()
            
            # æ„å»ºæœŸæƒä»£ç 
            option_ticker = self._construct_option_ticker(ticker, expiry_date, option_type, strike)
            
            return {
                'option_ticker': option_ticker,
                'ticker': ticker,
                'strike': strike,
                'option_type': option_type,
                'expiry': expiry_date,
                'signal_time_et': signal_time_et,
                'signal_date_et': signal_time_et.date()
            }
            
        except Exception as e:
            logger.debug(f"è§£æå¤±è´¥: {e}")
            return None
    
    def _construct_option_ticker(self, underlying: str, expiry, option_type: str, strike: float) -> str:
        """æ„å»ºPolygonæœŸæƒä»£ç """
        date_str = expiry.strftime('%y%m%d')
        cp = 'C' if option_type.lower() == 'call' else 'P'
        strike_str = f"{int(strike * 1000):08d}"
        return f"O:{underlying}{date_str}{cp}{strike_str}"
    
    def check_cache_exists(self, option_ticker: str, start_date, end_date) -> Dict:
        """
        æ£€æŸ¥ç¼“å­˜ä¸­å·²æœ‰å“ªäº›æ—¥æœŸçš„æ•°æ®
        
        Returns:
            {'cached_dates': [...], 'missing_dates': [...]}
        """
        all_dates = pd.date_range(start_date, end_date, freq='D')
        cached_dates = []
        missing_dates = []
        
        for date in all_dates:
            cache_file = self.cache_dir / f"{option_ticker.replace(':', '_')}_{date.date().isoformat()}.parquet"
            if cache_file.exists():
                cached_dates.append(date.date())
            else:
                missing_dates.append(date.date())
        
        return {
            'cached_dates': cached_dates,
            'missing_dates': missing_dates
        }
    
    def download_option_data(self, option_ticker: str, start_date, end_date) -> bool:
        """
        ä¸‹è½½æœŸæƒæ•°æ®å¹¶ä¿å­˜åˆ°ç¼“å­˜
        
        Args:
            option_ticker: æœŸæƒä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
        
        Returns:
            True if successful, False otherwise
        """
        try:
            start_str = start_date.isoformat() if hasattr(start_date, 'isoformat') else str(start_date)
            end_str = end_date.isoformat() if hasattr(end_date, 'isoformat') else str(end_date)
            
            # Polygon API endpoint
            url = (f"{self.BASE_URL}/{option_ticker}/range/1/minute"
                   f"/{start_str}/{end_str}"
                   f"?adjusted=true&sort=asc&limit=50000&apiKey={self.api_key}")
            
            self.api_calls += 1
            logger.info(f"ğŸ“¥ Downloading {option_ticker} ({start_str} to {end_str}) [API #{self.api_calls}]...")
            
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            if data.get('resultsCount', 0) == 0:
                logger.warning(f"âš ï¸  No data for {option_ticker} in range {start_str} to {end_str}")
                self.failed_tickers.append(option_ticker)
                return False
            
            results = data.get('results', [])
            records = []
            
            for item in results:
                timestamp = datetime.fromtimestamp(item['t'] / 1000, tz=pytz.UTC)
                timestamp = timestamp.astimezone(self.et_tz)
                
                records.append({
                    'datetime': timestamp,
                    'open': item['o'],
                    'high': item['h'],
                    'low': item['l'],
                    'close': item['c'],
                    'volume': item.get('v', 0),
                })
            
            if not records:
                logger.warning(f"âš ï¸  No records parsed for {option_ticker}")
                return False
            
            df = pd.DataFrame(records)
            df.set_index('datetime', inplace=True)
            
            # æŒ‰å¤©æ‹†åˆ†å¹¶ä¿å­˜
            days_saved = 0
            for day in pd.date_range(start_str, end_str, freq='D'):
                day_data = df[df.index.date == day.date()]
                if len(day_data) > 0:
                    self._save_to_cache(option_ticker, day.date(), day_data)
                    days_saved += 1
            
            logger.info(f"âœ… Saved {option_ticker}: {len(records)} bars â†’ {days_saved} days")
            return True
            
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 404:
                logger.warning(f"âš ï¸  {option_ticker} not found (404)")
            else:
                logger.warning(f"âš ï¸  HTTP error: {e}")
            self.failed_tickers.append(option_ticker)
            return False
        except Exception as e:
            logger.error(f"âŒ Failed to download {option_ticker}: {e}")
            self.failed_tickers.append(option_ticker)
            return False
    
    def _save_to_cache(self, option_ticker: str, date, df: pd.DataFrame):
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
        try:
            date_str = date.isoformat() if hasattr(date, 'isoformat') else str(date)
            cache_file = self.cache_dir / f"{option_ticker.replace(':', '_')}_{date_str}.parquet"
            
            # Reset index to save datetime as column
            df_to_save = df.reset_index()
            df_to_save.to_parquet(cache_file, index=False)
            
        except Exception as e:
            logger.debug(f"Failed to save cache: {e}")
    
    def prefetch_all(self, max_workers: int = 5, start_date_filter: str = "2024-01-01"):
        """
        é¢„ä¸‹è½½æ‰€æœ‰æœŸæƒæ•°æ®ï¼ˆæ”¯æŒå¹¶å‘ï¼‰
        
        Args:
            max_workers: å¹¶å‘çº¿ç¨‹æ•°ï¼ˆå»ºè®®3-10ï¼Œé¿å…è§¦å‘é™æµï¼‰
            start_date_filter: åªä¸‹è½½æ­¤æ—¥æœŸä¹‹åçš„æœŸæƒï¼ˆé»˜è®¤2024-01-01ï¼‰
        """
        logger.info("\n" + "="*60)
        logger.info("ğŸš€ å¼€å§‹é¢„ä¸‹è½½æ‰€æœ‰æœŸæƒæ•°æ®")
        logger.info("="*60)
        logger.info(f"ğŸ“… æ—¥æœŸè¿‡æ»¤: åªä¸‹è½½ {start_date_filter} åŠä¹‹åçš„æœŸæƒ")
        
        # è½¬æ¢è¿‡æ»¤æ—¥æœŸ
        filter_date = datetime.strptime(start_date_filter, '%Y-%m-%d').date()
        
        # åŠ è½½ä¿¡å·
        signals = self.load_option_signals()
        
        if not signals:
            logger.warning("âš ï¸  æ²¡æœ‰æ‰¾åˆ°æœŸæƒä¿¡å·")
            return
        
        # è¿‡æ»¤ï¼šåªä¿ç•™ä¿¡å·æ—¶é—´åœ¨è¿‡æ»¤æ—¥æœŸä¹‹åçš„
        filtered_signals = [sig for sig in signals if sig['signal_date_et'] >= filter_date]
        logger.info(f"ğŸ“Š è¿‡æ»¤åä¿¡å·æ•°: {len(signals)} â†’ {len(filtered_signals)}")
        
        # å»é‡ï¼šåŒä¸€ä¸ªæœŸæƒåªä¸‹è½½ä¸€æ¬¡
        unique_options = {}
        for sig in filtered_signals:
            option_ticker = sig['option_ticker']
            if option_ticker not in unique_options:
                unique_options[option_ticker] = sig
            else:
                # å–æœ€æ—©çš„ä¿¡å·æ—¶é—´ï¼ˆä½†ä¸æ—©äºè¿‡æ»¤æ—¥æœŸï¼‰
                if sig['signal_date_et'] < unique_options[option_ticker]['signal_date_et']:
                    unique_options[option_ticker] = sig
        
        logger.info(f"ğŸ“Š å»é‡åå…± {len(unique_options)} ä¸ªå”¯ä¸€æœŸæƒ")
        
        # æ£€æŸ¥å·²æœ‰ç¼“å­˜
        tasks = []
        for option_ticker, sig in unique_options.items():
            start_date = sig['signal_date_et']
            end_date = sig['expiry']
            
            # ç¡®ä¿å¼€å§‹æ—¥æœŸä¸æ—©äºè¿‡æ»¤æ—¥æœŸ
            if start_date < filter_date:
                start_date = filter_date
            
            cache_status = self.check_cache_exists(option_ticker, start_date, end_date)
            
            if len(cache_status['missing_dates']) == 0:
                logger.info(f"âœ… {option_ticker} ç¼“å­˜å·²å®Œæ•´ï¼Œè·³è¿‡")
                self.cache_hits += 1
            else:
                tasks.append({
                    'option_ticker': option_ticker,
                    'start_date': start_date,
                    'end_date': end_date,
                    'cached_days': len(cache_status['cached_dates']),
                    'missing_days': len(cache_status['missing_dates'])
                })
        
        logger.info(f"ğŸ“‹ éœ€è¦ä¸‹è½½çš„æœŸæƒæ•°: {len(tasks)}")
        logger.info(f"ğŸ’¾ å·²æœ‰å®Œæ•´ç¼“å­˜: {self.cache_hits} ä¸ª")
        
        if not tasks:
            logger.info("ğŸ‰ æ‰€æœ‰æœŸæƒæ•°æ®å·²ç¼“å­˜ï¼Œæ— éœ€ä¸‹è½½ï¼")
            return
        
        # å¹¶å‘ä¸‹è½½
        logger.info(f"\nğŸ”„ å¼€å§‹å¹¶å‘ä¸‹è½½ (å¹¶å‘æ•°: {max_workers})...\n")
        
        success_count = 0
        failed_count = 0
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(
                    self.download_option_data,
                    task['option_ticker'],
                    task['start_date'],
                    task['end_date']
                ): task for task in tasks
            }
            
            for idx, future in enumerate(as_completed(futures), 1):
                task = futures[future]
                try:
                    result = future.result()
                    if result:
                        success_count += 1
                    else:
                        failed_count += 1
                    
                    # è¿›åº¦æ˜¾ç¤º
                    if idx % 10 == 0 or idx == len(tasks):
                        logger.info(
                            f"â³ è¿›åº¦: {idx}/{len(tasks)} "
                            f"({idx/len(tasks)*100:.1f}%) | "
                            f"æˆåŠŸ: {success_count} | å¤±è´¥: {failed_count}"
                        )
                
                except Exception as e:
                    failed_count += 1
                    logger.error(f"âŒ Download task failed: {e}")
        
        # ç»Ÿè®¡ç»“æœ
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š ä¸‹è½½å®Œæˆç»Ÿè®¡")
        logger.info("="*60)
        logger.info(f"âœ… æˆåŠŸä¸‹è½½: {success_count} ä¸ªæœŸæƒ")
        logger.info(f"âŒ ä¸‹è½½å¤±è´¥: {failed_count} ä¸ªæœŸæƒ")
        logger.info(f"ğŸ’¾ å·²æœ‰ç¼“å­˜: {self.cache_hits} ä¸ªæœŸæƒ")
        logger.info(f"ğŸ“ APIè°ƒç”¨æ•°: {self.api_calls}")
        logger.info(f"ğŸ“ ç¼“å­˜ç›®å½•: {self.cache_dir}")
        
        if self.failed_tickers:
            logger.warning(f"\nâš ï¸  ä»¥ä¸‹ {len(self.failed_tickers)} ä¸ªæœŸæƒä¸‹è½½å¤±è´¥:")
            for ticker in self.failed_tickers[:20]:  # åªæ˜¾ç¤ºå‰20ä¸ª
                logger.warning(f"  - {ticker}")
            if len(self.failed_tickers) > 20:
                logger.warning(f"  ... è¿˜æœ‰ {len(self.failed_tickers) - 20} ä¸ª")
        
        logger.info("\nğŸ‰ é¢„ä¸‹è½½å®Œæˆï¼")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æœŸæƒæ•°æ®é¢„ä¸‹è½½ - æ‰¹é‡ä¸‹è½½CSVä¸­æ‰€æœ‰æœŸæƒçš„å†å²æ•°æ®')
    parser.add_argument(
        '--csv', 
        default='future_v_0_1/database/merged_strategy_v1_calls_bell_2023M3_2025M10.csv',
        help='CSVä¿¡å·æ–‡ä»¶è·¯å¾„'
    )
    parser.add_argument(
        '--cache-dir',
        default='future_v_0_1/database/option_cache',
        help='ç¼“å­˜ç›®å½•'
    )
    parser.add_argument(
        '--workers',
        type=int,
        default=5,
        help='å¹¶å‘ä¸‹è½½çº¿ç¨‹æ•°ï¼ˆ1-10ï¼Œå»ºè®®5ï¼‰'
    )
    parser.add_argument(
        '--start-date',
        type=str,
        default='2024-01-01',
        help='åªä¸‹è½½æ­¤æ—¥æœŸåŠä¹‹åçš„æœŸæƒæ•°æ®ï¼ˆé»˜è®¤2024-01-01ï¼Œé¿å…403é”™è¯¯ï¼‰'
    )
    
    args = parser.parse_args()
    
    print("\n" + "="*60)
    print("ğŸ“¥ æœŸæƒæ•°æ®é¢„ä¸‹è½½ç³»ç»Ÿ")
    print("="*60)
    print(f"ğŸ“‚ CSVæ–‡ä»¶: {args.csv}")
    print(f"ğŸ“ ç¼“å­˜ç›®å½•: {args.cache_dir}")
    print(f"ğŸ”„ å¹¶å‘æ•°: {args.workers}")
    print(f"ğŸ“… èµ·å§‹æ—¥æœŸ: {args.start_date} åŠä¹‹å")
    print("="*60 + "\n")
    
    # åˆ›å»ºé¢„ä¸‹è½½å™¨
    prefetcher = OptionDataPrefetcher(
        csv_file=args.csv,
        cache_dir=args.cache_dir
    )
    
    # å¼€å§‹ä¸‹è½½
    start_time = datetime.now()
    logger.info(f"ğŸš€ å¼€å§‹æ—¶é—´: {start_time}")
    
    try:
        prefetcher.prefetch_all(max_workers=args.workers, start_date_filter=args.start_date)
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸  ç”¨æˆ·ä¸­æ–­ä¸‹è½½")
        print("\nâš ï¸  ä¸‹è½½å·²ä¸­æ–­")
    except Exception as e:
        logger.error(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        print(f"\nâŒ ä¸‹è½½å¤±è´¥: {e}")
    
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    
    logger.info(f"âœ… ç»“æŸæ—¶é—´: {end_time}")
    logger.info(f"â±ï¸  æ€»ç”¨æ—¶: {duration:.1f} ç§’ ({duration/60:.1f} åˆ†é’Ÿ)")
    
    print("\n" + "="*60)
    print("âœ… é¢„ä¸‹è½½å®Œæˆï¼")
    print(f"â±ï¸  ç”¨æ—¶: {duration:.1f} ç§’ ({duration/60:.1f} åˆ†é’Ÿ)")
    print(f"ğŸ“ ç¼“å­˜ä½ç½®: {args.cache_dir}")
    print("="*60 + "\n")


if __name__ == '__main__':
    main()

